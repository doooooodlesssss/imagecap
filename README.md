# üß† Spectral-Guided Multimodal Image Captioning with LLM Refinement  
### *Integrating Frequency-Domain Fusion and Spectral Attention for Enhanced Visual-Linguistic Reasoning*

---

## üìò Overview
This repository presents **Spectral Captioning**, a novel deep learning framework for **image caption generation** that unifies spatial reasoning, frequency-domain fusion, and large language model (LLM) refinement.

The model integrates:

- **YOLOv8** ‚Üí object-level semantics (what is present, where it is)
- **Xception** ‚Üí scene-level features (global layout, context)
- **Dual Attention** ‚Üí independent attention for each modality  
- **DCT Frequency Fusion** ‚Üí noise-robust global structural fusion  
- **Spectral Attention** ‚Üí FFT-based global dependency modeling  
- **GRU Decoder** ‚Üí caption generation  
- **LLM Refinement** ‚Üí final grammar, richness, factual grounding  

This architecture is designed for **stronger grounding**, **better compositionality**, and **efficient long-range feature interaction**.

---

## üß© Architecture Overview

### **1Ô∏è‚É£ Dual Visual Feature Extraction**
- **YOLOv8**: detects objects, bounding boxes, class labels, and confidence scores  
- **Xception**: extracts high-level semantic context  

These together encode both **local details** and **global scene information**.

---

### **2Ô∏è‚É£ Dual Attention Refinement**
Each feature stream is passed through a **Bahdanau-style attention** module separately.

This prevents feature interference and allows:
- object attention ‚Üí focuses on key entities  
- scene attention ‚Üí focuses on high-level context  

---

### **3Ô∏è‚É£ DCT Frequency-Domain Fusion**
Steps:
1. Apply **2D DCT** on both attended feature maps  
2. Retain **low-frequency components** (rich in structure)  
3. **Elementwise multiply** the frequency maps  
4. Apply **inverse DCT** to reconstruct a fused, global-aware feature  

This improves robustness against noise and lighting variations.

---

### **4Ô∏è‚É£ Spectral Attention (Core Novelty)**
Instead of self-attention (O(n¬≤)), we use:

- **FFT ‚Üí learnable complex spectral weights ‚Üí inverse FFT**  

This yields:
- global receptive field  
- cheaper computation (O(n log n))  
- smoother, more coherent feature propagation  

---

### **5Ô∏è‚É£ GRU Caption Decoder**
A lightweight GRU sequentially generates a caption from the fused spectral features.

Trained with:
- Cross Entropy Loss  
- CIDEr optimization  
- Teacher forcing + scheduled sampling  

---

### **6Ô∏è‚É£ LLM-Guided Caption Refinement**
A multimodal LLM (MiniGPT-4 / LLaVA-style) receives:
- detected objects  
- raw GRU caption  
- image embeddings  

It refines the caption by:
- adding missing attributes  
- removing hallucinations  
- improving grammar and coherence  

This produces **human-like, grounded captions**.
